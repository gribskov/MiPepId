import ML
import sys
import pickle
import numpy as np
"""
LogisticRegression(C=10000.0, class_weight=None, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
[[ -5.57759579e+00  -1.05859130e+02  -4.04099724e+01  -4.73003909e+01
   -1.15944023e+01  -1.35499324e+02  -6.96921599e+01  -5.05889625e+01
    1.06909132e+01  -1.13998582e+02  -4.36024991e+01  -6.08945003e+01
    3.32641905e+01  -9.52333770e+01  -4.25772483e+01  -4.01171853e+01
   -8.74991254e+00  -1.28555437e+02  -6.63367327e+01  -4.85316525e+01
   -7.27756568e+00  -1.31792063e+02  -5.95844301e+01  -4.74150789e+01
    5.83648035e+00  -1.19405397e+02  -4.75617470e+01  -2.09248189e+01
   -6.45712491e+00  -1.17808779e+02  -6.12794174e+01  -4.39675011e+01
    1.01396323e+01  -1.14291203e+02  -5.12373807e+01  -5.56194754e+01
    1.00624046e+01  -1.08579545e+02  -5.75149113e+01  -4.85631936e+01
    2.00764723e+01  -1.23102599e+02  -3.97539501e+01  -3.15539007e+01
    1.18977732e+01  -9.15854740e+01  -4.00769470e+01  -3.79413458e+01
    3.75096913e+00  -1.06887052e+02  -4.98366159e+01  -4.20672905e+01
   -2.23796586e+01  -1.07964664e+02  -6.21945424e+01  -6.48052671e+01
   -1.12948130e+01  -9.65093727e+01  -5.58185035e+01  -3.37525318e+01
    5.98103136e+00  -9.96903732e+01  -6.33551089e+01  -7.55357475e+01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.15003833e+02   2.19461057e-01   8.03316537e+01   6.25697377e+01
    1.43243972e+02   1.14703945e+01   8.63523395e+01   8.25277112e+01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.18865768e+02  -7.75475006e+00   5.41687205e+01   4.44312565e+01
    1.12756381e+02  -5.02439599e+00   7.28463421e+01   6.38045711e+01
    1.29379677e+02  -4.58037434e+00   5.53781537e+01   9.46703031e+01
    1.11493114e+02  -3.51149548e+00   5.87383416e+01   6.28688072e+01
    1.22052654e+02  -1.00137602e+01   5.84068672e+01   5.51824638e+01
    1.11169944e+02   1.65807172e+01   3.87404617e+01   5.94441954e+01
    1.31421290e+02  -1.27773744e+01   4.91866494e+01   6.43869315e+01
    1.21636222e+02   3.93777797e+01   6.92523571e+01   4.05135279e+01
    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
    1.06669167e+02  -1.84793880e+01   3.47484935e+01   4.72586934e+01
    1.12854641e+02   8.59001107e+00   5.48507321e+01   3.63704528e+01
    1.14273590e+02   1.14382809e+00   5.96244323e+01   4.20256539e+01
    5.36960710e+01  -6.10556143e+01  -3.08631437e+00   3.79123320e+00
    3.50447529e+01  -6.50109528e+01   9.75187382e+00  -1.45064740e+00
    5.28125614e+01  -6.21375842e+01  -5.16742564e+00  -6.09211764e-01
    7.05775912e+01  -5.24637963e+01   6.37673108e+00   8.11037559e+00
    4.47679038e+01  -5.81192484e+01  -1.33135841e+01   6.95529030e+00
    4.95470920e+01  -6.48223723e+01  -8.70891870e+00  -1.90696891e+01
    6.08267031e+01  -5.98868339e+01  -1.07505246e+01   5.42047226e-01
    5.28722573e+01  -3.37377870e+01  -2.15058102e+00   5.62076932e+00
    7.42038831e+01  -7.30633343e+01  -1.03414191e+00   4.03259620e-01
    6.25798033e+01  -6.20342963e+01  -2.48348527e+00  -1.28989162e+00
    4.67894663e+01  -5.85141036e+01  -7.96355984e+00   8.79433715e+00
    5.69606600e+01  -6.63854369e+01   3.27504433e+01   9.62616425e-01
    5.62602889e+01  -6.52692461e+01   8.62970241e+00  -8.18086603e-01
    7.54924311e+01  -5.47740270e+01   2.00177003e+01   9.08236915e+00
    6.93157465e+01  -5.19226615e+01   4.08674764e+00  -1.44607527e+01
    8.05478099e+01  -6.51419322e+00   4.40737781e+01   3.15309602e+00
    6.00953853e+01  -5.66429293e+01   1.35737043e+01   9.15312420e+00
    6.59706127e+01  -6.62643170e+01   1.84054952e+01   4.27073472e-01
    5.33166663e+01  -4.97146256e+01   2.56342582e+01   1.90745293e+01
    6.49879744e+01  -5.90936211e+01   1.92314208e+00   1.59903275e+01
    6.15077598e+01  -5.70131969e+01  -9.08418837e+00  -2.28102018e+00
    5.35815571e+01  -7.11274886e+01  -3.37726880e+00  -4.75182346e+00
    6.47758366e+01  -6.90522469e+01  -5.16711990e-02   3.48228733e+00
    4.91129306e+01  -6.29615636e+01  -1.69272582e+01  -8.06704690e+00
    4.44769390e+01  -7.25176651e+01  -1.38187706e+01   7.99919964e+00
    5.80777355e+01  -6.39614145e+01   2.70245140e+01  -5.80524646e+00
    5.12273533e+01  -5.99319562e+01   2.08131354e+01   3.44488487e+01
    7.76892172e+01  -7.27316393e+01   1.77333630e+01   3.55487592e+01
    5.80743102e+01  -5.34346189e+01   1.43489816e+01  -2.44180727e-01
    4.13457278e+01  -4.81705250e+01  -5.30251955e+00   8.11774427e+00
    4.92675520e+01  -5.87421900e+01   2.33518040e+01   4.45204265e+00
    4.75086432e+01  -5.84129208e+01  -8.79602240e+00  -3.60929319e+00]]
0.6
"""
from sklearn.linear_model import LogisticRegression

print(sys.version_info)
logr = LogisticRegression(C=10000.0, class_weight=None, dual=False,
                          fit_intercept=True, intercept_scaling=1, max_iter=100,
                          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
threshold = 0.6
coef = np.array([[ -5.57759579e+00, -1.05859130e+02, -4.04099724e+01, -4.73003909e+01,
   -1.15944023e+01, -1.35499324e+02, -6.96921599e+01, -5.05889625e+01,
    1.06909132e+01, -1.13998582e+02, -4.36024991e+01, -6.08945003e+01,
    3.32641905e+01, -9.52333770e+01, -4.25772483e+01, -4.01171853e+01,
   -8.74991254e+00, -1.28555437e+02, -6.63367327e+01, -4.85316525e+01,
   -7.27756568e+00, -1.31792063e+02, -5.95844301e+01, -4.74150789e+01,
    5.83648035e+00, -1.19405397e+02, -4.75617470e+01, -2.09248189e+01,
   -6.45712491e+00, -1.17808779e+02, -6.12794174e+01, -4.39675011e+01,
    1.01396323e+01, -1.14291203e+02, -5.12373807e+01, -5.56194754e+01,
    1.00624046e+01, -1.08579545e+02, -5.75149113e+01, -4.85631936e+01,
    2.00764723e+01, -1.23102599e+02, -3.97539501e+01, -3.15539007e+01,
    1.18977732e+01, -9.15854740e+01, -4.00769470e+01, -3.79413458e+01,
    3.75096913e+00, -1.06887052e+02, -4.98366159e+01, -4.20672905e+01,
   -2.23796586e+01, -1.07964664e+02, -6.21945424e+01, -6.48052671e+01,
   -1.12948130e+01, -9.65093727e+01, -5.58185035e+01, -3.37525318e+01,
    5.98103136e+00, -9.96903732e+01, -6.33551089e+01, -7.55357475e+01,
    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
    1.15003833e+02,  2.19461057e-01,  8.03316537e+01,  6.25697377e+01,
    1.43243972e+02,  1.14703945e+01,  8.63523395e+01,  8.25277112e+01,
    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
    1.18865768e+02, -7.75475006e+00,  5.41687205e+01,  4.44312565e+01,
    1.12756381e+02, -5.02439599e+00,  7.28463421e+01,  6.38045711e+01,
    1.29379677e+02, -4.58037434e+00,  5.53781537e+01,  9.46703031e+01,
    1.11493114e+02, -3.51149548e+00,  5.87383416e+01,  6.28688072e+01,
    1.22052654e+02, -1.00137602e+01,  5.84068672e+01,  5.51824638e+01,
    1.11169944e+02,  1.65807172e+01,  3.87404617e+01,  5.94441954e+01,
    1.31421290e+02, -1.27773744e+01,  4.91866494e+01,  6.43869315e+01,
    1.21636222e+02,  3.93777797e+01,  6.92523571e+01,  4.05135279e+01,
    0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
    1.06669167e+02, -1.84793880e+01,  3.47484935e+01,  4.72586934e+01,
    1.12854641e+02,  8.59001107e+00,  5.48507321e+01,  3.63704528e+01,
    1.14273590e+02,  1.14382809e+00,  5.96244323e+01,  4.20256539e+01,
    5.36960710e+01, -6.10556143e+01, -3.08631437e+00,  3.79123320e+00,
    3.50447529e+01, -6.50109528e+01,  9.75187382e+00, -1.45064740e+00,
    5.28125614e+01, -6.21375842e+01, -5.16742564e+00, -6.09211764e-01,
    7.05775912e+01, -5.24637963e+01,  6.37673108e+00,  8.11037559e+00,
    4.47679038e+01, -5.81192484e+01, -1.33135841e+01,  6.95529030e+00,
    4.95470920e+01, -6.48223723e+01, -8.70891870e+00, -1.90696891e+01,
    6.08267031e+01, -5.98868339e+01, -1.07505246e+01,  5.42047226e-01,
    5.28722573e+01, -3.37377870e+01, -2.15058102e+00,  5.62076932e+00,
    7.42038831e+01, -7.30633343e+01, -1.03414191e+00,  4.03259620e-01,
    6.25798033e+01, -6.20342963e+01, -2.48348527e+00, -1.28989162e+00,
    4.67894663e+01, -5.85141036e+01, -7.96355984e+00,  8.79433715e+00,
    5.69606600e+01, -6.63854369e+01,  3.27504433e+01,  9.62616425e-01,
    5.62602889e+01, -6.52692461e+01,  8.62970241e+00, -8.18086603e-01,
    7.54924311e+01, -5.47740270e+01,  2.00177003e+01,  9.08236915e+00,
    6.93157465e+01, -5.19226615e+01,  4.08674764e+00, -1.44607527e+01,
    8.05478099e+01, -6.51419322e+00,  4.40737781e+01,  3.15309602e+00,
    6.00953853e+01, -5.66429293e+01,  1.35737043e+01,  9.15312420e+00,
    6.59706127e+01, -6.62643170e+01,  1.84054952e+01,  4.27073472e-01,
    5.33166663e+01, -4.97146256e+01,  2.56342582e+01,  1.90745293e+01,
    6.49879744e+01, -5.90936211e+01,  1.92314208e+00,  1.59903275e+01,
    6.15077598e+01, -5.70131969e+01, -9.08418837e+00, -2.28102018e+00,
    5.35815571e+01, -7.11274886e+01, -3.37726880e+00, -4.75182346e+00,
    6.47758366e+01, -6.90522469e+01, -5.16711990e-02,   3.48228733e+00,
    4.91129306e+01, -6.29615636e+01, -1.69272582e+01, -8.06704690e+00,
    4.44769390e+01, -7.25176651e+01, -1.38187706e+01,  7.99919964e+00,
    5.80777355e+01, -6.39614145e+01,  2.70245140e+01, -5.80524646e+00,
    5.12273533e+01, -5.99319562e+01,  2.08131354e+01,  3.44488487e+01,
    7.76892172e+01, -7.27316393e+01,  1.77333630e+01,  3.55487592e+01,
    5.80743102e+01, -5.34346189e+01,  1.43489816e+01, -2.44180727e-01,
    4.13457278e+01, -4.81705250e+01, -5.30251955e+00,  8.11774427e+00,
    4.92675520e+01, -5.87421900e+01,  2.33518040e+01,  4.45204265e+00,
    4.75086432e+01, -5.84129208e+01, -8.79602240e+00, -3.60929319e+00]])

# coef.shape = (1,256)
logr.coef_ = coef
logr.intercept_ = np.array([3.89456584])
print(logr.coef_.shape)
print(logr.coef_)
save = open('newmodel.sav', 'wb')
pickle.dump(logr, save)
pickle.dump(threshold, save)
save.close()

exit(0)
